{".class": "MypyFile", "_fullname": "pyspark.sql.streaming.readwriter", "future_import_flags": [], "is_partial_stub_package": false, "is_stub": false, "names": {".class": "SymbolTable", "Any": {".class": "SymbolTableNode", "cross_ref": "typing.Any", "kind": "Gdef", "module_public": false}, "Callable": {".class": "SymbolTableNode", "cross_ref": "typing.Callable", "kind": "Gdef", "module_public": false}, "DataFrame": {".class": "SymbolTableNode", "cross_ref": "pyspark.sql.dataframe.DataFrame", "kind": "Gdef", "module_public": false}, "DataStreamReader": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "TypeInfo", "_promote": [], "abstract_attributes": [], "bases": ["pyspark.sql.readwriter.OptionUtils"], "declared_metaclass": null, "defn": {".class": "ClassDef", "fullname": "pyspark.sql.streaming.readwriter.DataStreamReader", "name": "DataStreamReader", "type_vars": []}, "deletable_attributes": [], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamReader", "has_param_spec_type": false, "metaclass_type": null, "metadata": {}, "module_name": "pyspark.sql.streaming.readwriter", "mro": ["pyspark.sql.streaming.readwriter.DataStreamReader", "pyspark.sql.readwriter.OptionUtils", "builtins.object"], "names": {".class": "SymbolTable", "__init__": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "spark"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamReader.__init__", "name": "__init__", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "spark"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamReader", "pyspark.sql.session.SparkSession"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "__init__ of DataStreamReader", "ret_type": {".class": "NoneType"}, "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "_df": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "jdf"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamReader._df", "name": "_df", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "jdf"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamReader", {".class": "AnyType", "missing_import_name": "pyspark.sql.streaming.readwriter.JavaObject", "source_any": null, "type_of_any": 3}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "_df of DataStreamReader", "ret_type": "pyspark.sql.dataframe.DataFrame", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "_jreader": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamReader._jreader", "name": "_jreader", "type": {".class": "AnyType", "missing_import_name": "pyspark.sql.pandas.conversion.JavaObject", "source_any": {".class": "AnyType", "missing_import_name": "pyspark.sql.pandas.conversion.JavaObject", "source_any": null, "type_of_any": 3}, "type_of_any": 7}}}, "_spark": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamReader._spark", "name": "_spark", "type": "pyspark.sql.session.SparkSession"}}, "csv": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "arg_names": ["self", "path", "schema", "sep", "encoding", "quote", "escape", "comment", "header", "inferSchema", "ignoreLeadingWhiteSpace", "ignoreTrailingWhiteSpace", "nullValue", "nanValue", "positiveInf", "negativeInf", "dateFormat", "timestampFormat", "maxColumns", "maxCharsPerColumn", "maxMalformedLogPerPartition", "mode", "columnNameOfCorruptRecord", "multiLine", "charToEscapeQuoteEscaping", "enforceSchema", "emptyValue", "locale", "lineSep", "pathGlobFilter", "recursiveFileLookup", "unescapedQuoteHandling"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamReader.csv", "name": "csv", "type": {".class": "CallableType", "arg_kinds": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "arg_names": ["self", "path", "schema", "sep", "encoding", "quote", "escape", "comment", "header", "inferSchema", "ignoreLeadingWhiteSpace", "ignoreTrailingWhiteSpace", "nullValue", "nanValue", "positiveInf", "negativeInf", "dateFormat", "timestampFormat", "maxColumns", "maxCharsPerColumn", "maxMalformedLogPerPartition", "mode", "columnNameOfCorruptRecord", "multiLine", "charToEscapeQuoteEscaping", "enforceSchema", "emptyValue", "locale", "lineSep", "pathGlobFilter", "recursiveFileLookup", "unescapedQuoteHandling"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamReader", "builtins.str", {".class": "UnionType", "items": ["pyspark.sql.types.StructType", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.int", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.int", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.int", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "csv of DataStreamReader", "ret_type": "pyspark.sql.dataframe.DataFrame", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "format": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "source"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamReader.format", "name": "format", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "source"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamReader", "builtins.str"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "format of DataStreamReader", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamReader", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "json": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "arg_names": ["self", "path", "schema", "primitivesAsString", "prefersDecimal", "allowComments", "allowUnquotedFieldNames", "allowSingleQuotes", "allowNumericLeadingZero", "allowBackslashEscapingAnyCharacter", "mode", "columnNameOfCorruptRecord", "dateFormat", "timestampFormat", "multiLine", "allowUnquotedControlChars", "lineSep", "locale", "dropFieldIfAllNull", "encoding", "pathGlobFilter", "recursiveFileLookup", "allowNonNumericNumbers"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamReader.json", "name": "json", "type": {".class": "CallableType", "arg_kinds": [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "arg_names": ["self", "path", "schema", "primitivesAsString", "prefersDecimal", "allowComments", "allowUnquotedFieldNames", "allowSingleQuotes", "allowNumericLeadingZero", "allowBackslashEscapingAnyCharacter", "mode", "columnNameOfCorruptRecord", "dateFormat", "timestampFormat", "multiLine", "allowUnquotedControlChars", "lineSep", "locale", "dropFieldIfAllNull", "encoding", "pathGlobFilter", "recursiveFileLookup", "allowNonNumericNumbers"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamReader", "builtins.str", {".class": "UnionType", "items": ["pyspark.sql.types.StructType", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "json of DataStreamReader", "ret_type": "pyspark.sql.dataframe.DataFrame", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "load": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 1, 1, 1, 4], "arg_names": ["self", "path", "format", "schema", "options"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamReader.load", "name": "load", "type": {".class": "CallableType", "arg_kinds": [0, 1, 1, 1, 4], "arg_names": ["self", "path", "format", "schema", "options"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamReader", {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["pyspark.sql.types.StructType", "builtins.str", {".class": "NoneType"}]}, {".class": "TypeAliasType", "args": [], "type_ref": "pyspark.sql._typing.OptionalPrimitiveType"}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "load of DataStreamReader", "ret_type": "pyspark.sql.dataframe.DataFrame", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "option": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 0], "arg_names": ["self", "key", "value"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamReader.option", "name": "option", "type": {".class": "CallableType", "arg_kinds": [0, 0, 0], "arg_names": ["self", "key", "value"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamReader", "builtins.str", {".class": "TypeAliasType", "args": [], "type_ref": "pyspark.sql._typing.OptionalPrimitiveType"}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "option of DataStreamReader", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamReader", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "options": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 4], "arg_names": ["self", "options"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamReader.options", "name": "options", "type": {".class": "CallableType", "arg_kinds": [0, 4], "arg_names": ["self", "options"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamReader", {".class": "TypeAliasType", "args": [], "type_ref": "pyspark.sql._typing.OptionalPrimitiveType"}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "options of DataStreamReader", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamReader", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "orc": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 1, 1, 1], "arg_names": ["self", "path", "mergeSchema", "pathGlobFilter", "recursiveFileLookup"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamReader.orc", "name": "orc", "type": {".class": "CallableType", "arg_kinds": [0, 0, 1, 1, 1], "arg_names": ["self", "path", "mergeSchema", "pathGlobFilter", "recursiveFileLookup"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamReader", "builtins.str", {".class": "UnionType", "items": ["builtins.bool", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "orc of DataStreamReader", "ret_type": "pyspark.sql.dataframe.DataFrame", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "parquet": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 1, 1, 1, 1, 1], "arg_names": ["self", "path", "mergeSchema", "pathGlobFilter", "recursiveFileLookup", "datetimeRebaseMode", "int96RebaseMode"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamReader.parquet", "name": "parquet", "type": {".class": "CallableType", "arg_kinds": [0, 0, 1, 1, 1, 1, 1], "arg_names": ["self", "path", "mergeSchema", "pathGlobFilter", "recursiveFileLookup", "datetimeRebaseMode", "int96RebaseMode"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamReader", "builtins.str", {".class": "UnionType", "items": ["builtins.bool", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "parquet of DataStreamReader", "ret_type": "pyspark.sql.dataframe.DataFrame", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "schema": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "schema"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamReader.schema", "name": "schema", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "schema"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamReader", {".class": "UnionType", "items": ["pyspark.sql.types.StructType", "builtins.str"]}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "schema of DataStreamReader", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamReader", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "table": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "tableName"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamReader.table", "name": "table", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "tableName"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamReader", "builtins.str"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "table of DataStreamReader", "ret_type": "pyspark.sql.dataframe.DataFrame", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "text": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 1, 1, 1, 1], "arg_names": ["self", "path", "wholetext", "lineSep", "pathGlobFilter", "recursiveFileLookup"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamReader.text", "name": "text", "type": {".class": "CallableType", "arg_kinds": [0, 0, 1, 1, 1, 1], "arg_names": ["self", "path", "wholetext", "lineSep", "pathGlobFilter", "recursiveFileLookup"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamReader", "builtins.str", "builtins.bool", {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", "builtins.str", {".class": "NoneType"}]}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "text of DataStreamReader", "ret_type": "pyspark.sql.dataframe.DataFrame", "type_guard": null, "unpack_kwargs": false, "variables": []}}}}, "slots": null, "tuple_type": null, "type_vars": [], "typeddict_type": null}}, "DataStreamWriter": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "TypeInfo", "_promote": [], "abstract_attributes": [], "bases": ["builtins.object"], "declared_metaclass": null, "defn": {".class": "ClassDef", "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter", "name": "DataStreamWriter", "type_vars": []}, "deletable_attributes": [], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter", "has_param_spec_type": false, "metaclass_type": null, "metadata": {}, "module_name": "pyspark.sql.streaming.readwriter", "mro": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "builtins.object"], "names": {".class": "SymbolTable", "__init__": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "df"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.__init__", "name": "__init__", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "df"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "pyspark.sql.dataframe.DataFrame"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "__init__ of DataStreamWriter", "ret_type": {".class": "NoneType"}, "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "_df": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter._df", "name": "_df", "type": "pyspark.sql.dataframe.DataFrame"}}, "_jwrite": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter._jwrite", "name": "_jwrite", "type": {".class": "AnyType", "missing_import_name": "pyspark.sql.dataframe.JavaObject", "source_any": {".class": "AnyType", "missing_import_name": "pyspark.sql.dataframe.JavaObject", "source_any": null, "type_of_any": 3}, "type_of_any": 7}}}, "_spark": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter._spark", "name": "_spark", "type": "pyspark.sql.session.SparkSession"}}, "_sq": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "jsq"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter._sq", "name": "_sq", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "jsq"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", {".class": "AnyType", "missing_import_name": "pyspark.sql.streaming.readwriter.JavaObject", "source_any": null, "type_of_any": 3}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "_sq of DataStreamWriter", "ret_type": "pyspark.sql.streaming.query.StreamingQuery", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "foreach": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "OverloadedFuncDef", "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.foreach", "impl": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "f"], "flags": ["is_overload"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.foreach", "name": "foreach", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "f"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", {".class": "UnionType", "items": [{".class": "CallableType", "arg_kinds": [0], "arg_names": [null], "arg_types": ["pyspark.sql.types.Row"], "bound_args": [], "def_extras": {}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": null, "ret_type": {".class": "NoneType"}, "type_guard": null, "unpack_kwargs": false, "variables": []}, "pyspark.sql._typing.SupportsProcess"]}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "foreach of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}}, "items": [{".class": "Decorator", "func": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "f"], "flags": ["is_overload", "is_decorated"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.foreach", "name": "foreach", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "f"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", {".class": "CallableType", "arg_kinds": [0], "arg_names": [null], "arg_types": ["pyspark.sql.types.Row"], "bound_args": [], "def_extras": {}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": null, "ret_type": {".class": "NoneType"}, "type_guard": null, "unpack_kwargs": false, "variables": []}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "foreach of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}}, "is_overload": true, "var": {".class": "Var", "flags": ["is_inferred"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.foreach", "name": "foreach", "type": null}}, {".class": "Decorator", "func": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "f"], "flags": ["is_overload", "is_decorated"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.foreach", "name": "foreach", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "f"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "pyspark.sql._typing.SupportsProcess"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "foreach of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}}, "is_overload": true, "var": {".class": "Var", "flags": ["is_inferred"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.foreach", "name": "foreach", "type": null}}], "type": {".class": "Overloaded", "items": [{".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "f"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", {".class": "CallableType", "arg_kinds": [0], "arg_names": [null], "arg_types": ["pyspark.sql.types.Row"], "bound_args": [], "def_extras": {}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": null, "ret_type": {".class": "NoneType"}, "type_guard": null, "unpack_kwargs": false, "variables": []}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "foreach of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}, {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "f"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "pyspark.sql._typing.SupportsProcess"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "foreach of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}]}}}, "foreachBatch": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "func"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.foreachBatch", "name": "foreachBatch", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "func"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": [null, null], "arg_types": ["pyspark.sql.dataframe.DataFrame", "builtins.int"], "bound_args": [], "def_extras": {}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": null, "ret_type": {".class": "NoneType"}, "type_guard": null, "unpack_kwargs": false, "variables": []}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "foreachBatch of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "format": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "source"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.format", "name": "format", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "source"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "builtins.str"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "format of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "option": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 0], "arg_names": ["self", "key", "value"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.option", "name": "option", "type": {".class": "CallableType", "arg_kinds": [0, 0, 0], "arg_names": ["self", "key", "value"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "builtins.str", {".class": "TypeAliasType", "args": [], "type_ref": "pyspark.sql._typing.OptionalPrimitiveType"}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "option of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "options": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 4], "arg_names": ["self", "options"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.options", "name": "options", "type": {".class": "CallableType", "arg_kinds": [0, 4], "arg_names": ["self", "options"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", {".class": "TypeAliasType", "args": [], "type_ref": "pyspark.sql._typing.OptionalPrimitiveType"}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "options of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "outputMode": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "outputMode"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.outputMode", "name": "outputMode", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "outputMode"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "builtins.str"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "outputMode of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "partitionBy": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "OverloadedFuncDef", "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.partitionBy", "impl": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 2], "arg_names": ["self", "cols"], "flags": ["is_overload"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.partitionBy", "name": "partitionBy", "type": {".class": "CallableType", "arg_kinds": [0, 2], "arg_names": ["self", "cols"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "builtins.str"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "partitionBy of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}}, "items": [{".class": "Decorator", "func": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 2], "arg_names": ["self", "cols"], "flags": ["is_overload", "is_decorated"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.partitionBy", "name": "partitionBy", "type": {".class": "CallableType", "arg_kinds": [0, 2], "arg_names": ["self", "cols"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "builtins.str"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "partitionBy of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}}, "is_overload": true, "var": {".class": "Var", "flags": ["is_inferred"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.partitionBy", "name": "partitionBy", "type": null}}, {".class": "Decorator", "func": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", null], "flags": ["is_overload", "is_decorated"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.partitionBy", "name": "partitionBy", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", null], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", {".class": "Instance", "args": ["builtins.str"], "type_ref": "builtins.list"}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "partitionBy of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}}, "is_overload": true, "var": {".class": "Var", "flags": ["is_inferred"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.partitionBy", "name": "partitionBy", "type": null}}], "type": {".class": "Overloaded", "items": [{".class": "CallableType", "arg_kinds": [0, 2], "arg_names": ["self", "cols"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "builtins.str"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "partitionBy of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}, {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", null], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", {".class": "Instance", "args": ["builtins.str"], "type_ref": "builtins.list"}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "partitionBy of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}]}}}, "queryName": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "queryName"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.queryName", "name": "queryName", "type": {".class": "CallableType", "arg_kinds": [0, 0], "arg_names": ["self", "queryName"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "builtins.str"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "queryName of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "start": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 1, 1, 1, 1, 1, 4], "arg_names": ["self", "path", "format", "outputMode", "partitionBy", "queryName", "options"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.start", "name": "start", "type": {".class": "CallableType", "arg_kinds": [0, 1, 1, 1, 1, 1, 4], "arg_names": ["self", "path", "format", "outputMode", "partitionBy", "queryName", "options"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "Instance", "args": ["builtins.str"], "type_ref": "builtins.list"}, {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "TypeAliasType", "args": [], "type_ref": "pyspark.sql._typing.OptionalPrimitiveType"}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "start of DataStreamWriter", "ret_type": "pyspark.sql.streaming.query.StreamingQuery", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "toTable": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 1, 1, 1, 1, 4], "arg_names": ["self", "tableName", "format", "outputMode", "partitionBy", "queryName", "options"], "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.toTable", "name": "toTable", "type": {".class": "CallableType", "arg_kinds": [0, 0, 1, 1, 1, 1, 4], "arg_names": ["self", "tableName", "format", "outputMode", "partitionBy", "queryName", "options"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "builtins.str", {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "Instance", "args": ["builtins.str"], "type_ref": "builtins.list"}, {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "TypeAliasType", "args": [], "type_ref": "pyspark.sql._typing.OptionalPrimitiveType"}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "toTable of DataStreamWriter", "ret_type": "pyspark.sql.streaming.query.StreamingQuery", "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "trigger": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "OverloadedFuncDef", "flags": [], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.trigger", "impl": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 5, 5, 5, 5], "arg_names": ["self", "processingTime", "once", "continuous", "availableNow"], "flags": ["is_overload"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.trigger", "name": "trigger", "type": {".class": "CallableType", "arg_kinds": [0, 5, 5, 5, 5], "arg_names": ["self", "processingTime", "once", "continuous", "availableNow"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.str", {".class": "NoneType"}]}, {".class": "UnionType", "items": ["builtins.bool", {".class": "NoneType"}]}], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "trigger of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}}, "items": [{".class": "Decorator", "func": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 3], "arg_names": ["self", "processingTime"], "flags": ["is_overload", "is_decorated"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.trigger", "name": "trigger", "type": {".class": "CallableType", "arg_kinds": [0, 3], "arg_names": ["self", "processingTime"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "builtins.str"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "trigger of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}}, "is_overload": true, "var": {".class": "Var", "flags": ["is_inferred"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.trigger", "name": "trigger", "type": null}}, {".class": "Decorator", "func": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 3], "arg_names": ["self", "once"], "flags": ["is_overload", "is_decorated"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.trigger", "name": "trigger", "type": {".class": "CallableType", "arg_kinds": [0, 3], "arg_names": ["self", "once"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "builtins.bool"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "trigger of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}}, "is_overload": true, "var": {".class": "Var", "flags": ["is_inferred"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.trigger", "name": "trigger", "type": null}}, {".class": "Decorator", "func": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 3], "arg_names": ["self", "continuous"], "flags": ["is_overload", "is_decorated"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.trigger", "name": "trigger", "type": {".class": "CallableType", "arg_kinds": [0, 3], "arg_names": ["self", "continuous"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "builtins.str"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "trigger of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}}, "is_overload": true, "var": {".class": "Var", "flags": ["is_inferred"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.trigger", "name": "trigger", "type": null}}, {".class": "Decorator", "func": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 3], "arg_names": ["self", "availableNow"], "flags": ["is_overload", "is_decorated"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.trigger", "name": "trigger", "type": {".class": "CallableType", "arg_kinds": [0, 3], "arg_names": ["self", "availableNow"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "builtins.bool"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "trigger of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}}, "is_overload": true, "var": {".class": "Var", "flags": ["is_inferred"], "fullname": "pyspark.sql.streaming.readwriter.DataStreamWriter.trigger", "name": "trigger", "type": null}}], "type": {".class": "Overloaded", "items": [{".class": "CallableType", "arg_kinds": [0, 3], "arg_names": ["self", "processingTime"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "builtins.str"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "trigger of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}, {".class": "CallableType", "arg_kinds": [0, 3], "arg_names": ["self", "once"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "builtins.bool"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "trigger of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}, {".class": "CallableType", "arg_kinds": [0, 3], "arg_names": ["self", "continuous"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "builtins.str"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "trigger of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}, {".class": "CallableType", "arg_kinds": [0, 3], "arg_names": ["self", "availableNow"], "arg_types": ["pyspark.sql.streaming.readwriter.DataStreamWriter", "builtins.bool"], "bound_args": [], "def_extras": {"first_arg": "self"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "trigger of DataStreamWriter", "ret_type": "pyspark.sql.streaming.readwriter.DataStreamWriter", "type_guard": null, "unpack_kwargs": false, "variables": []}]}}}}, "slots": null, "tuple_type": null, "type_vars": [], "typeddict_type": null}}, "ForeachBatchFunction": {".class": "SymbolTableNode", "cross_ref": "pyspark.sql.utils.ForeachBatchFunction", "kind": "Gdef", "module_public": false}, "Iterator": {".class": "SymbolTableNode", "cross_ref": "typing.Iterator", "kind": "Gdef", "module_public": false}, "JavaObject": {".class": "SymbolTableNode", "kind": "Gdef", "module_public": false, "node": {".class": "Var", "flags": ["is_suppressed_import", "is_ready", "is_inferred"], "fullname": "pyspark.sql.streaming.readwriter.JavaObject", "name": "JavaObject", "type": {".class": "AnyType", "missing_import_name": "pyspark.sql.streaming.readwriter.JavaObject", "source_any": null, "type_of_any": 3}}}, "List": {".class": "SymbolTableNode", "cross_ref": "typing.List", "kind": "Gdef", "module_public": false}, "OptionUtils": {".class": "SymbolTableNode", "cross_ref": "pyspark.sql.readwriter.OptionUtils", "kind": "Gdef", "module_public": false}, "Optional": {".class": "SymbolTableNode", "cross_ref": "typing.Optional", "kind": "Gdef", "module_public": false}, "OptionalPrimitiveType": {".class": "SymbolTableNode", "cross_ref": "pyspark.sql._typing.OptionalPrimitiveType", "kind": "Gdef", "module_public": false}, "Row": {".class": "SymbolTableNode", "cross_ref": "pyspark.sql.types.Row", "kind": "Gdef", "module_public": false}, "SparkSession": {".class": "SymbolTableNode", "cross_ref": "pyspark.sql.session.SparkSession", "kind": "Gdef", "module_public": false}, "StreamingQuery": {".class": "SymbolTableNode", "cross_ref": "pyspark.sql.streaming.query.StreamingQuery", "kind": "Gdef", "module_public": false}, "StructType": {".class": "SymbolTableNode", "cross_ref": "pyspark.sql.types.StructType", "kind": "Gdef", "module_public": false}, "SupportsProcess": {".class": "SymbolTableNode", "cross_ref": "pyspark.sql._typing.SupportsProcess", "kind": "Gdef", "module_public": false}, "TYPE_CHECKING": {".class": "SymbolTableNode", "cross_ref": "typing.TYPE_CHECKING", "kind": "Gdef", "module_public": false}, "Union": {".class": "SymbolTableNode", "cross_ref": "typing.Union", "kind": "Gdef", "module_public": false}, "__all__": {".class": "SymbolTableNode", "kind": "Gdef", "module_public": false, "node": {".class": "Var", "flags": ["is_inferred", "has_explicit_value"], "fullname": "pyspark.sql.streaming.readwriter.__all__", "name": "__all__", "type": {".class": "Instance", "args": ["builtins.str"], "type_ref": "builtins.list"}}}, "__annotations__": {".class": "SymbolTableNode", "kind": "Gdef", "module_public": false, "node": {".class": "Var", "flags": ["is_ready"], "fullname": "pyspark.sql.streaming.readwriter.__annotations__", "name": "__annotations__", "type": {".class": "Instance", "args": ["builtins.str", {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 6}], "type_ref": "builtins.dict"}}}, "__doc__": {".class": "SymbolTableNode", "kind": "Gdef", "module_public": false, "node": {".class": "Var", "flags": ["is_ready"], "fullname": "pyspark.sql.streaming.readwriter.__doc__", "name": "__doc__", "type": "builtins.str"}}, "__file__": {".class": "SymbolTableNode", "kind": "Gdef", "module_public": false, "node": {".class": "Var", "flags": ["is_ready"], "fullname": "pyspark.sql.streaming.readwriter.__file__", "name": "__file__", "type": "builtins.str"}}, "__name__": {".class": "SymbolTableNode", "kind": "Gdef", "module_public": false, "node": {".class": "Var", "flags": ["is_ready"], "fullname": "pyspark.sql.streaming.readwriter.__name__", "name": "__name__", "type": "builtins.str"}}, "__package__": {".class": "SymbolTableNode", "kind": "Gdef", "module_public": false, "node": {".class": "Var", "flags": ["is_ready"], "fullname": "pyspark.sql.streaming.readwriter.__package__", "name": "__package__", "type": "builtins.str"}}, "_test": {".class": "SymbolTableNode", "kind": "Gdef", "module_public": false, "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [], "arg_names": [], "flags": [], "fullname": "pyspark.sql.streaming.readwriter._test", "name": "_test", "type": {".class": "CallableType", "arg_kinds": [], "arg_names": [], "arg_types": [], "bound_args": [], "def_extras": {"first_arg": null}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "_test", "ret_type": {".class": "NoneType"}, "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "_to_seq": {".class": "SymbolTableNode", "cross_ref": "pyspark.sql.column._to_seq", "kind": "Gdef", "module_public": false}, "cast": {".class": "SymbolTableNode", "cross_ref": "typing.cast", "kind": "Gdef", "module_public": false}, "java_import": {".class": "SymbolTableNode", "kind": "Gdef", "module_public": false, "node": {".class": "Var", "flags": ["is_suppressed_import", "is_ready", "is_inferred"], "fullname": "pyspark.sql.streaming.readwriter.java_import", "name": "java_import", "type": {".class": "AnyType", "missing_import_name": "pyspark.sql.streaming.readwriter.java_import", "source_any": null, "type_of_any": 3}}}, "overload": {".class": "SymbolTableNode", "cross_ref": "typing.overload", "kind": "Gdef", "module_public": false}, "sys": {".class": "SymbolTableNode", "cross_ref": "sys", "kind": "Gdef", "module_public": false}, "to_str": {".class": "SymbolTableNode", "cross_ref": "pyspark.sql.utils.to_str", "kind": "Gdef", "module_public": false}}, "path": "C:\\Users\\Egor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\streaming\\readwriter.py"}